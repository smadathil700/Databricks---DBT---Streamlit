{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc4112d2-cf72-49b4-9308-269f6c699f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc06b319-3f0f-4adf-944c-e07116cd5214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /local_disk0/.ephemeral_nfs/envs/pythonEnv-99d0e525-a23a-4323-9806-3e58706db6a8/lib/python3.11/site-packages (0.0.7)\n",
      "Collecting dbt-core\n",
      "  Downloading dbt_core-1.10.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dbt-databricks\n",
      "  Downloading dbt_databricks-1.10.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /databricks/python3/lib/python3.11/site-packages (from ucimlrepo) (1.5.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /databricks/python3/lib/python3.11/site-packages (from ucimlrepo) (2023.7.22)\n",
      "Collecting agate<1.10,>=1.7.0 (from dbt-core)\n",
      "  Downloading agate-1.9.1-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting Jinja2<4,>=3.1.3 (from dbt-core)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting mashumaro<3.15,>=3.9 (from mashumaro[msgpack]<3.15,>=3.9->dbt-core)\n",
      "  Downloading mashumaro-3.14-py3-none-any.whl.metadata (114 kB)\n",
      "Requirement already satisfied: click<9.0,>=8.0.2 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (8.0.4)\n",
      "Collecting jsonschema<5.0,>=4.19.1 (from dbt-core)\n",
      "  Downloading jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting networkx<4.0,>=2.3 (from dbt-core)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: protobuf<6.0,>=5.0 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (5.29.3)\n",
      "Requirement already satisfied: requests<3.0.0 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (2.31.0)\n",
      "Collecting snowplow-tracker<2.0,>=1.0.2 (from dbt-core)\n",
      "  Downloading snowplow_tracker-1.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: pathspec<0.13,>=0.9 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (0.10.3)\n",
      "Requirement already satisfied: sqlparse<0.6.0,>=0.5.0 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (0.5.1)\n",
      "Collecting dbt-extractor<=0.6,>=0.5.0 (from dbt-core)\n",
      "  Downloading dbt_extractor-0.6.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.6 kB)\n",
      "Collecting dbt-semantic-interfaces<0.9,>=0.8.3 (from dbt-core)\n",
      "  Downloading dbt_semantic_interfaces-0.8.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting dbt-common<2.0,>=1.25.1 (from dbt-core)\n",
      "  Downloading dbt_common-1.25.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting dbt-adapters<2.0,>=1.15.2 (from dbt-core)\n",
      "  Downloading dbt_adapters-1.16.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting dbt-protos<2.0,>=1.0.315 (from dbt-core)\n",
      "  Downloading dbt_protos-1.0.335-py3-none-any.whl.metadata (859 bytes)\n",
      "Requirement already satisfied: pydantic<3 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (1.10.6)\n",
      "Requirement already satisfied: packaging>20.9 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (23.2)\n",
      "Requirement already satisfied: pytz>=2015.7 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (2022.7)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (6.0)\n",
      "Collecting daff>=1.3.46 (from dbt-core)\n",
      "  Downloading daff-1.4.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.4 in /databricks/python3/lib/python3.11/site-packages (from dbt-core) (4.10.0)\n",
      "Collecting databricks-sdk<0.48.0,>=0.41 (from dbt-databricks)\n",
      "  Downloading databricks_sdk-0.47.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting databricks-sql-connector<5.0.0,>=4.0.0 (from databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks)\n",
      "  Downloading databricks_sql_connector-4.0.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting dbt-spark<2.0,>=1.9.0 (from dbt-databricks)\n",
      "  Downloading dbt_spark-1.9.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting keyring>=23.13.0 (from dbt-databricks)\n",
      "  Downloading keyring-25.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting Babel>=2.0 (from agate<1.10,>=1.7.0->dbt-core)\n",
      "  Downloading babel-2.17.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: isodate>=0.5.4 in /databricks/python3/lib/python3.11/site-packages (from agate<1.10,>=1.7.0->dbt-core) (0.7.2)\n",
      "Collecting leather>=0.3.2 (from agate<1.10,>=1.7.0->dbt-core)\n",
      "  Downloading leather-0.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting parsedatetime!=2.5,>=2.1 (from agate<1.10,>=1.7.0->dbt-core)\n",
      "  Downloading parsedatetime-2.6-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting python-slugify>=1.2.1 (from agate<1.10,>=1.7.0->dbt-core)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pytimeparse>=1.1.5 (from agate<1.10,>=1.7.0->dbt-core)\n",
      "  Downloading pytimeparse-1.1.8-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk<0.48.0,>=0.41->dbt-databricks) (2.35.0)\n",
      "Collecting lz4<5.0.0,>=4.0.2 (from databricks-sql-connector<5.0.0,>=4.0.0->databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks)\n",
      "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: oauthlib<4.0.0,>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-sql-connector<5.0.0,>=4.0.0->databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks) (3.2.0)\n",
      "Collecting openpyxl<4.0.0,>=3.0.10 (from databricks-sql-connector<5.0.0,>=4.0.0->databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks)\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sql-connector<5.0.0,>=4.0.0->databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks) (2.8.2)\n",
      "Collecting thrift<0.21.0,>=0.16.0 (from databricks-sql-connector<5.0.0,>=4.0.0->databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks)\n",
      "  Downloading thrift-0.20.0.tar.gz (62 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: urllib3>=1.26 in /databricks/python3/lib/python3.11/site-packages (from databricks-sql-connector<5.0.0,>=4.0.0->databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks) (1.26.16)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in /databricks/python3/lib/python3.11/site-packages (from databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks) (14.0.1)\n",
      "Collecting colorama<0.5,>=0.3.9 (from dbt-common<2.0,>=1.25.1->dbt-core)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting deepdiff<8.0,>=7.0 (from dbt-common<2.0,>=1.25.1->dbt-core)\n",
      "  Downloading deepdiff-7.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting isodate>=0.5.4 (from agate<1.10,>=1.7.0->dbt-core)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: importlib-metadata<9,>=6.0 in /databricks/python3/lib/python3.11/site-packages (from dbt-semantic-interfaces<0.9,>=0.8.3->dbt-core) (6.0.0)\n",
      "Requirement already satisfied: more-itertools<11.0,>=8.0 in /usr/lib/python3/dist-packages (from dbt-semantic-interfaces<0.9,>=0.8.3->dbt-core) (8.10.0)\n",
      "Collecting sqlparams>=3.0.0 (from dbt-spark<2.0,>=1.9.0->dbt-databricks)\n",
      "  Downloading sqlparams-6.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2<4,>=3.1.3->dbt-core)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema<5.0,>=4.19.1->dbt-core)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0,>=4.19.1->dbt-core)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0,>=4.19.1->dbt-core)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0,>=4.19.1->dbt-core)\n",
      "  Downloading rpds_py-0.26.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /usr/lib/python3/dist-packages (from keyring>=23.13.0->dbt-databricks) (3.3.1)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /usr/lib/python3/dist-packages (from keyring>=23.13.0->dbt-databricks) (0.7.1)\n",
      "Collecting jaraco.classes (from keyring>=23.13.0->dbt-databricks)\n",
      "  Downloading jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring>=23.13.0->dbt-databricks)\n",
      "  Downloading jaraco_functools-4.2.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jaraco.context (from keyring>=23.13.0->dbt-databricks)\n",
      "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting msgpack>=0.5.6 (from mashumaro[msgpack]<3.15,>=3.9->dbt-core)\n",
      "  Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /databricks/python3/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (1.23.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0->dbt-core) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests<3.0.0->dbt-core) (3.4)\n",
      "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff<8.0,>=7.0->dbt-common<2.0,>=1.25.1->dbt-core)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<0.48.0,>=0.41->dbt-databricks) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<0.48.0,>=0.41->dbt-databricks) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<0.48.0,>=0.41->dbt-databricks) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib-metadata<9,>=6.0->dbt-semantic-interfaces<0.9,>=0.8.3->dbt-core) (3.11.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from isodate>=0.5.4->agate<1.10,>=1.7.0->dbt-core) (1.16.0)\n",
      "Collecting et-xmlfile (from openpyxl<4.0.0,>=3.0.10->databricks-sql-connector<5.0.0,>=4.0.0->databricks-sql-connector[pyarrow]<5.0.0,>=4.0.0->dbt-databricks)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify>=1.2.1->agate<1.10,>=1.7.0->dbt-core)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting backports.tarfile (from jaraco.context->keyring>=23.13.0->dbt-databricks)\n",
      "  Downloading backports.tarfile-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<0.48.0,>=0.41->dbt-databricks) (0.4.8)\n",
      "Downloading dbt_core-1.10.3-py3-none-any.whl (977 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/977.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m977.3/977.3 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dbt_databricks-1.10.4-py3-none-any.whl (126 kB)\n",
      "Downloading agate-1.9.1-py2.py3-none-any.whl (95 kB)\n",
      "Downloading daff-1.4.2-py3-none-any.whl (144 kB)\n",
      "Downloading databricks_sdk-0.47.0-py3-none-any.whl (681 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/681.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m681.0/681.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading databricks_sql_connector-4.0.5-py3-none-any.whl (133 kB)\n",
      "Downloading dbt_adapters-1.16.0-py3-none-any.whl (166 kB)\n",
      "Downloading dbt_common-1.25.1-py3-none-any.whl (85 kB)\n",
      "Downloading dbt_extractor-0.6.0-cp39-abi3-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (435 kB)\n",
      "Downloading dbt_protos-1.0.335-py3-none-any.whl (91 kB)\n",
      "Downloading dbt_semantic_interfaces-0.8.5-py3-none-any.whl (146 kB)\n",
      "Downloading dbt_spark-1.9.2-py3-none-any.whl (50 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading jsonschema-4.24.0-py3-none-any.whl (88 kB)\n",
      "Downloading keyring-25.6.0-py3-none-any.whl (39 kB)\n",
      "Downloading mashumaro-3.14-py3-none-any.whl (92 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading snowplow_tracker-1.1.0-py3-none-any.whl (44 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading babel-2.17.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/10.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m204.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
      "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Downloading leather-0.4.0-py2.py3-none-any.whl (30 kB)\n",
      "Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.2 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (23 kB)\n",
      "Downloading msgpack-1.1.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (423 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading parsedatetime-2.6-py3-none-any.whl (42 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading pytimeparse-1.1.8-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.26.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (381 kB)\n",
      "Downloading sqlparams-6.2.0-py3-none-any.whl (17 kB)\n",
      "Downloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco_functools-4.2.1-py3-none-any.whl (10 kB)\n",
      "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading backports.tarfile-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Building wheels for collected packages: thrift\n",
      "  Building wheel for thrift (setup.py): started\n",
      "  Building wheel for thrift (setup.py): finished with status 'done'\n",
      "  Created wheel for thrift: filename=thrift-0.20.0-cp311-cp311-linux_aarch64.whl size=415727 sha256=261d7917e4501904cf2d19e9ec06349e5986dbef7847f6e4879cbd948f886322\n",
      "  Stored in directory: /home/spark-99d0e525-a23a-4323-9806-3e/.cache/pip/wheels/e2/bc/11/b3431cfbf4afeb954480926b2f46fdc56082b8b234d6d4208b\n",
      "Successfully built thrift\n",
      "Installing collected packages: text-unidecode, pytimeparse, parsedatetime, leather, daff, thrift, sqlparams, rpds-py, python-slugify, ordered-set, networkx, msgpack, mashumaro, MarkupSafe, lz4, jaraco.functools, jaraco.classes, isodate, et-xmlfile, dbt-protos, dbt-extractor, colorama, backports.tarfile, Babel, attrs, snowplow-tracker, referencing, openpyxl, Jinja2, jaraco.context, deepdiff, agate, keyring, jsonschema-specifications, databricks-sql-connector, databricks-sdk, jsonschema, dbt-semantic-interfaces, dbt-common, dbt-adapters, dbt-core, dbt-spark, dbt-databricks\n",
      "  Attempting uninstall: isodate\n",
      "    Found existing installation: isodate 0.7.2\n",
      "    Not uninstalling isodate at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99d0e525-a23a-4323-9806-3e58706db6a8\n",
      "    Can't uninstall 'isodate'. No files were found to uninstall.\n",
      "  Attempting uninstall: keyring\n",
      "    Found existing installation: keyring 23.5.0\n",
      "    Not uninstalling keyring at /usr/lib/python3/dist-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99d0e525-a23a-4323-9806-3e58706db6a8\n",
      "    Can't uninstall 'keyring'. No files were found to uninstall.\n",
      "  Attempting uninstall: databricks-sdk\n",
      "    Found existing installation: databricks-sdk 0.40.0\n",
      "    Not uninstalling databricks-sdk at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-99d0e525-a23a-4323-9806-3e58706db6a8\n",
      "    Can't uninstall 'databricks-sdk'. No files were found to uninstall.\n",
      "Successfully installed Babel-2.17.0 Jinja2-3.1.6 MarkupSafe-3.0.2 agate-1.9.1 attrs-25.3.0 backports.tarfile-1.2.0 colorama-0.4.6 daff-1.4.2 databricks-sdk-0.47.0 databricks-sql-connector-4.0.5 dbt-adapters-1.16.0 dbt-common-1.25.1 dbt-core-1.10.3 dbt-databricks-1.10.4 dbt-extractor-0.6.0 dbt-protos-1.0.335 dbt-semantic-interfaces-0.8.5 dbt-spark-1.9.2 deepdiff-7.0.1 et-xmlfile-2.0.0 isodate-0.6.1 jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.2.1 jsonschema-4.24.0 jsonschema-specifications-2025.4.1 keyring-25.6.0 leather-0.4.0 lz4-4.4.4 mashumaro-3.14 msgpack-1.1.1 networkx-3.5 openpyxl-3.1.5 ordered-set-4.1.0 parsedatetime-2.6 python-slugify-8.0.4 pytimeparse-1.1.8 referencing-0.36.2 rpds-py-0.26.0 snowplow-tracker-1.1.0 sqlparams-6.2.0 text-unidecode-1.3 thrift-0.20.0\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install ucimlrepo dbt-core dbt-databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Load dataset\n",
    "ds = fetch_ucirepo(id=468)\n",
    "\n",
    "# Convert features and labels to pandas DataFrames (ensure columns are present)\n",
    "features_pdf = ds.data.features\n",
    "labels_pdf = ds.data.targets\n",
    "\n",
    "# If they are NumPy arrays, assign column names manually\n",
    "import pandas as pd\n",
    "\n",
    "if not hasattr(features_pdf, \"columns\"):\n",
    "    feature_names = ['Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration',\n",
    "                     'ProductRelated', 'ProductRelated_Duration', 'BounceRates', 'ExitRates',\n",
    "                     'PageValues', 'SpecialDay', 'Month', 'OperatingSystems', 'Browser', 'Region',\n",
    "                     'TrafficType', 'VisitorType', 'Weekend']\n",
    "    features_pdf = pd.DataFrame(features_pdf, columns=feature_names)\n",
    "\n",
    "if not hasattr(labels_pdf, \"columns\"):\n",
    "    labels_pdf = pd.DataFrame(labels_pdf, columns=['Revenue'])\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Convert to Spark DataFrames\n",
    "features_df = spark.createDataFrame(features_pdf)\n",
    "labels_df = spark.createDataFrame(labels_pdf)\n",
    "\n",
    "# Add DeviceType column\n",
    "features_df = features_df.withColumn(\n",
    "    \"DeviceType\",\n",
    "    when(col(\"OperatingSystems\").rlike(\"(?i)android|ios\"), \"mobile\")\n",
    "    .when(col(\"OperatingSystems\").rlike(\"(?i)windows|mac|linux\"), \"desktop\")\n",
    "    .otherwise(\"other\")\n",
    ")\n",
    "\n",
    "\n",
    "# Combine features and target using an artificial ID\n",
    "df = features_df.withColumn(\"id\", monotonically_increasing_id()) \\\n",
    "    .join(labels_df.withColumn(\"id\", monotonically_increasing_id()), on=\"id\") \\\n",
    "    .withColumnRenamed(\"id\", \"session_id\")\n",
    "\n",
    "# Define funnel columns\n",
    "spark_df = df.withColumn(\"product_view\", (col(\"ProductRelated\") > 0).cast(\"int\")) \\\n",
    "             .withColumn(\"add_to_cart\", (col(\"PageValues\") > 0).cast(\"int\")) \\\n",
    "             .withColumn(\"purchase\", col(\"Revenue\").cast(\"int\"))\n",
    "\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"default.source_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8394fa4-2b37-4504-a706-05806b6e7f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Load dataset\n",
    "ds = fetch_ucirepo(id=468)  # Online Shoppers Purchasing Intention Dataset\n",
    "\n",
    "# Create Spark DataFrames\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "features_df = spark.createDataFrame(ds.data.features)\n",
    "labels_df = spark.createDataFrame(ds.data.targets)\n",
    "\n",
    "# Combine features with the target (\"Revenue\")\n",
    "df = features_df.withColumn(\"id\", monotonically_increasing_id()) \\\n",
    "    .join(labels_df.withColumn(\"id\", monotonically_increasing_id()), on=\"id\") \\\n",
    "    .drop(\"id\")\n",
    "\n",
    "# Define funnel columns\n",
    "spark_df = df.withColumn(\"product_view\", (col(\"ProductRelated\")>0).cast(\"int\")) \\\n",
    "                   .withColumn(\"add_to_cart\", (col(\"PageValues\") > 0).cast(\"int\")) \\\n",
    "                   .withColumn(\"purchase\", col(\"Revenue\").cast(\"int\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a2bc460-4429-4f11-9452-5858dd398cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Website Visits</th><th>Product Views</th><th>Add to Cart</th><th>Purchases</th><th>Conversion Rate</th><th>Drop Off</th><th>Drop Off Rate</th></tr></thead><tbody><tr><td>12330</td><td>12292</td><td>2730</td><td>1908</td><td>0.15474452554744525</td><td>38</td><td>0.0030819140308191405</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         12330,
         12292,
         2730,
         1908,
         0.15474452554744525,
         38,
         0.0030819140308191405
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Website Visits",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Product Views",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Add to Cart",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Purchases",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Conversion Rate",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Drop Off",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Drop Off Rate",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>VisitorType</th><th>Purchases</th><th>Product Views</th><th>Add to Cart</th><th>Purchases</th><th>ConvRate</th></tr></thead><tbody><tr><td>New_Visitor</td><td>422</td><td>1687</td><td>381</td><td>422</td><td>0.25014819205690575</td></tr><tr><td>Returning_Visitor</td><td>1470</td><td>10520</td><td>2333</td><td>1470</td><td>0.1397338403041825</td></tr><tr><td>Other</td><td>16</td><td>85</td><td>16</td><td>16</td><td>0.18823529411764706</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "New_Visitor",
         422,
         1687,
         381,
         422,
         0.25014819205690575
        ],
        [
         "Returning_Visitor",
         1470,
         10520,
         2333,
         1470,
         0.1397338403041825
        ],
        [
         "Other",
         16,
         85,
         16,
         16,
         0.18823529411764706
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "VisitorType",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Purchases",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Product Views",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Add to Cart",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Purchases",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "ConvRate",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Month</th><th>(sum(purchase) / sum(product_view))</th></tr></thead><tbody><tr><td>Feb</td><td>0.01639344262295082</td></tr><tr><td>Mar</td><td>0.10084033613445378</td></tr><tr><td>May</td><td>0.10885773933790635</td></tr><tr><td>Aug</td><td>0.17592592592592593</td></tr><tr><td>Jul</td><td>0.1527777777777778</td></tr><tr><td>Nov</td><td>0.25452109845947757</td></tr><tr><td>Oct</td><td>0.21100917431192662</td></tr><tr><td>Sep</td><td>0.19239373601789708</td></tr><tr><td>June</td><td>0.10104529616724739</td></tr><tr><td>Dec</td><td>0.1253627394080093</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Feb",
         0.01639344262295082
        ],
        [
         "Mar",
         0.10084033613445378
        ],
        [
         "May",
         0.10885773933790635
        ],
        [
         "Aug",
         0.17592592592592593
        ],
        [
         "Jul",
         0.1527777777777778
        ],
        [
         "Nov",
         0.25452109845947757
        ],
        [
         "Oct",
         0.21100917431192662
        ],
        [
         "Sep",
         0.19239373601789708
        ],
        [
         "June",
         0.10104529616724739
        ],
        [
         "Dec",
         0.1253627394080093
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Month",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\": \"true\"}",
         "name": "(sum(purchase) / sum(product_view))",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>purchase</th><th>AvgBounce</th><th>AvgExit</th></tr></thead><tbody><tr><td>1</td><td>0.005117152640461216</td><td>0.019555168256813412</td></tr><tr><td>0</td><td>0.025317232197850686</td><td>0.04737827052648263</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         0.005117152640461216,
         0.019555168256813412
        ],
        [
         0,
         0.025317232197850686,
         0.04737827052648263
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "purchase",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "AvgBounce",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "AvgExit",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "funnel = spark_df.agg(\n",
    "    sum(\"product_view\").alias(\"Product Views\"),\n",
    "    sum(\"add_to_cart\").alias(\"Add to Cart\"),\n",
    "    sum(\"purchase\").alias(\"Purchases\")\n",
    ").toPandas()\n",
    "funnel.insert(0, \"Website Visits\", df.count())\n",
    "funnel[\"Conversion Rate\"] = funnel[\"Purchases\"] / funnel[\"Website Visits\"]\n",
    "funnel[\"Drop Off\"] = funnel[\"Website Visits\"].shift(0) - funnel[\"Product Views\"]  # adjust per row\n",
    "funnel[\"Drop Off Rate\"] = funnel[\"Drop Off\"] / funnel[\"Website Visits\"]\n",
    "display(funnel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c3ca940-7dc4-48e5-8fa9-946d48890a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Segmentation by VisitorType & Month\n",
    "seg = spark_df.groupBy(\"VisitorType\") \\\n",
    "    .agg(sum(\"purchase\").alias(\"Purchases\"), sum(\"product_view\").alias(\"Product Views\"), \n",
    "         sum(\"add_to_cart\").alias(\"Add to Cart\"), sum(\"purchase\").alias(\"Purchases\"), \n",
    "         sum(\"purchase\") / sum(\"product_view\")).withColumnRenamed(\"(sum(purchase) / sum(product_view))\", \"ConvRate\")\n",
    "display(seg)\n",
    "\n",
    "seg_m = spark_df.groupBy(\"Month\") \\\n",
    "    .agg(sum(\"purchase\") / sum(\"product_view\")).withColumnRenamed(\"((sum(purchase) / sum(product_view)))\", \"ConvRate\")\n",
    "display(seg_m)\n",
    "\n",
    "# Cell 6: Bounce & Exit rates vs purchase\n",
    "from pyspark.sql.functions import avg\n",
    "spark_df.groupBy(\"purchase\") \\\n",
    "  .agg(avg(\"BounceRates\").alias(\"AvgBounce\"), avg(\"ExitRates\").alias(\"AvgExit\")) \\\n",
    "  .display()\n",
    "\n",
    "# Cell 7: Save cleaned data to Delta for dbt\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.online_shopping_intention_cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Load dataset\n",
    "ds = fetch_ucirepo(id=468)  # Online Shoppers Purchasing Intention Dataset\n",
    "\n",
    "# Create Spark DataFrames\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "features_df = spark.createDataFrame(ds.data.features)\n",
    "labels_df = spark.createDataFrame(ds.data.targets)\n",
    "\n",
    "# Combine features with the target (\"Revenue\")\n",
    "df = features_df.withColumn(\"id\", monotonically_increasing_id()) \\\n",
    "    .join(labels_df.withColumn(\"id\", monotonically_increasing_id()), on=\"id\") \\\n",
    "    .drop(\"id\")\n",
    "\n",
    "# Define funnel columns\n",
    "spark_df = df.withColumn(\"product_view\", (col(\"ProductRelated\")>0).cast(\"int\")) \\\n",
    "                   .withColumn(\"add_to_cart\", (col(\"PageValues\") > 0).cast(\"int\")) \\\n",
    "                   .withColumn(\"purchase\", col(\"Revenue\").cast(\"int\"))\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "funnel = spark_df.agg(\n",
    "    sum(\"product_view\").alias(\"Product Views\"),\n",
    "    sum(\"add_to_cart\").alias(\"Add to Cart\"),\n",
    "    sum(\"purchase\").alias(\"Purchases\")\n",
    ").toPandas()\n",
    "funnel.insert(0, \"Website Visits\", df.count())\n",
    "\n",
    "funnel[\"Conversion Rate\"] = funnel[\"Purchases\"] / funnel[\"Website Visits\"]\n",
    "funnel[\"Drop Off\"] = funnel[\"Website Visits\"].shift(0) - funnel[\"Product Views\"]  # adjust per row\n",
    "funnel[\"Drop Off Rate\"] = funnel[\"Drop Off\"] / funnel[\"Website Visits\"]\n",
    "\n",
    "\n",
    "print(funnel.head(10))\n",
    "\n",
    "# Cell 5: Segmentation by VisitorType & Month\n",
    "seg = spark_df.groupBy(\"VisitorType\") \\\n",
    "    .agg(sum(\"purchase\").alias(\"Purchases\"), sum(\"product_view\").alias(\"Product Views\"), \n",
    "         sum(\"add_to_cart\").alias(\"Add to Cart\"), sum(\"purchase\").alias(\"Purchases\"), \n",
    "         sum(\"purchase\") / sum(\"product_view\")).withColumnRenamed(\"(sum(purchase) / sum(product_view))\", \"ConvRate\")\n",
    "display(seg)\n",
    "\n",
    "seg_m = spark_df.groupBy(\"Month\") \\\n",
    "    .agg(sum(\"purchase\") / sum(\"product_view\")).withColumnRenamed(\"((sum(purchase) / sum(product_view)))\", \"ConvRate\")\n",
    "display(seg_m)\n",
    "\n",
    "# Cell 6: Bounce & Exit rates vs purchase\n",
    "from pyspark.sql.functions import avg\n",
    "spark_df.groupBy(\"purchase\") \\\n",
    "  .agg(avg(\"BounceRates\").alias(\"AvgBounce\"), avg(\"ExitRates\").alias(\"AvgExit\")) \\\n",
    "  .display()\n",
    "\n",
    "# 1️⃣ Average Order Value (AOV) - Assuming 'Revenue' is present\n",
    "'''\n",
    "aov_df = spark_df.filter(col(\"purchase\") == 1) \\\n",
    "    .agg(round(avg(\"Revenue\"), 2).alias(\"Average Order Value\"))\n",
    "aov_df.display()\n",
    "'''\n",
    "\n",
    "# 2️⃣ Cart Abandonment Rate\n",
    "cart_abandonment_df = spark_df.agg(\n",
    "    (sum(col(\"add_to_cart\") - col(\"purchase\")) / sum(\"add_to_cart\")).alias(\"Cart Abandonment Rate\")\n",
    ")\n",
    "cart_abandonment_df.display()\n",
    "'''\n",
    "# 3️⃣ Estimated Total Revenue (if 'Revenue' or 'price' field exists)\n",
    "total_revenue_df = spark_df.agg(round(sum(\"Revenue\"), 2).alias(\"Total Revenue\"))\n",
    "total_revenue_df.display()\n",
    "'''\n",
    "# 4️⃣ Average Session Duration vs. Conversion\n",
    "session_vs_purchase_df = spark_df.groupBy(\"purchase\") \\\n",
    "    .agg(\n",
    "        round(avg(\"PageValues\"), 2).alias(\"AvgPageValue\"),\n",
    "        round(avg(\"Administrative_Duration\"), 2).alias(\"AvgAdminDuration\"),\n",
    "        round(avg(\"Informational_Duration\"), 2).alias(\"AvgInfoDuration\"),\n",
    "        round(avg(\"ProductRelated_Duration\"), 2).alias(\"AvgProductDuration\")\n",
    "    )\n",
    "session_vs_purchase_df.display()\n",
    "\n",
    "# 5️⃣ Device Category Segmentation (if device_type exists)\n",
    "if \"DeviceType\" in spark_df.columns:\n",
    "    device_seg = spark_df.groupBy(\"DeviceType\") \\\n",
    "        .agg(\n",
    "            sum(\"purchase\").alias(\"Purchases\"),\n",
    "            count(\"*\").alias(\"Sessions\"),\n",
    "            round(sum(\"purchase\") / count(\"*\"), 4).alias(\"ConvRate\")\n",
    "        )\n",
    "    device_seg.display()\n",
    "\n",
    "# 6️⃣ Returning vs New Visitor Conversion\n",
    "visitor_seg = spark_df.groupBy(\"VisitorType\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"Sessions\"),\n",
    "        sum(\"purchase\").alias(\"Purchases\"),\n",
    "        round(sum(\"purchase\") / count(\"*\"), 4).alias(\"ConvRate\")\n",
    "    )\n",
    "visitor_seg.display()\n",
    "\n",
    "# 7️⃣ Funnel by Region or Traffic Type (if available)\n",
    "if \"Region\" in spark_df.columns:\n",
    "    region_funnel = spark_df.groupBy(\"Region\") \\\n",
    "        .agg(\n",
    "            sum(\"product_view\").alias(\"Product Views\"),\n",
    "            sum(\"add_to_cart\").alias(\"Add to Cart\"),\n",
    "            sum(\"purchase\").alias(\"Purchases\"),\n",
    "            round(sum(\"purchase\") / count(\"*\"), 4).alias(\"ConvRate\")\n",
    "        )\n",
    "    region_funnel.display()\n",
    "\n",
    "\n",
    "# ========== Save Cleaned Data ==========\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.online_shopping_intention_cleaned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "UCI Data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
